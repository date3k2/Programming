{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# softmax stable function\n",
    "\n",
    "\n",
    "def stable_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def softmax_loss(X, y, W):\n",
    "    A = stable_softmax(X.dot(W))\n",
    "    idx = range(A.shape[0])\n",
    "    return -np.mean(np.log(A[idx, y]))+0.5*np.sum(W**2)*1/A.shape[0]\n",
    "\n",
    "\n",
    "def softmax_grad(X, y, W):\n",
    "    \"\"\"\n",
    "    W: 2d numpy array of shape (d, C),\n",
    "    each column correspoding to one output node\n",
    "    X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    y: 1d numpy array -- label of each row of X\n",
    "    \"\"\"\n",
    "    A = stable_softmax(X.dot(W))  # shape of (N, C)\n",
    "    id0 = range(X.shape[0])\n",
    "    A[id0, y] -= 1  # A - Y, shape of (N, C)\n",
    "    # return X.T.dot(A)/X.shape[0]\n",
    "    return A\n",
    "\n",
    "\n",
    "# test\n",
    "X = np.array([[3, 3, 5], [1, 2, -1], [3, 3, 5], [1, 2, -1]])\n",
    "y = np.array([0, 2, 1, 1])\n",
    "W = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "# print(softmax_loss(X, y, W))\n",
    "A = stable_softmax(X.dot(W))\n",
    "print(A)\n",
    "# print(np.argmax(A, axis=1))\n",
    "print(softmax_grad(X, y, W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network on MNIST using scikit-learn\n",
    "# Path: Python\\mnist_sklearn.ipynb\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000,random_state=3,alpha=0.001)\n",
    "start = time.time()\n",
    "mlp.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"Training time: \", end-start)\n",
    "predictions = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "joblib.dump(mlp, 'mlp.pkl')\n",
    "mlp = joblib.load('mlp.pkl')\n",
    "predictions = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "date3k2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
